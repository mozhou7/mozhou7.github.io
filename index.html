<html>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<head>
    <title>Mo Zhou's Homepage, Duke University</title>
    <style>
    #header {
        text-align: center;
        padding: 0px;
        margin-left: 150px;
    }

    #main {
        width:80%;
        float:left;
        padding:0px;
        margin-left:10%;        
    }
    #footer {
        clear:both;
        text-align:left;
        padding:5px;  
        margin-left:150px;     
    }
</style>
</head>

<body>
    <div id="header">
        <table cellspacing="25">
            <td>
                <img src="MoZhou.jpg" width="200" height="274">    
            </td>
            <td>
                <h2>Mo Zhou 周墨</h2>
                Postdoc<br>
                Institute for Foundations of Data Science (IFDS)<br>
                University of Washington<br>
                <br>
                Email: mozhou17 at cs dot washington dot edu<br>
                <a href="https://scholar.google.com/citations?user=j_SEFF8AAAAJ" target="_blank">
                Google scholar<br></a>
                <br>
            </td>
        </table>
    </div>

    <div id="main">
        <p>
            I am a postdoc at <a href="https://ifds.info/" target='_blank'> Institute for Foundations of Data Science (IFDS)</a> at University of Washington, where I'm working with <a href="https://simonshaoleidu.com/" target="_blank"> Simon Du</a> and <a href="https://people.ece.uw.edu/fazel_maryam/" target="_blank"> Maryam Fazel</a>. 

            Previously, I received my Ph.D. in Computer Science from Duke University, where I was fortunate to be advised by <a href="https://users.cs.duke.edu/~rongge/" target="_blank"> Rong Ge</a>. Before coming to Duke, I received B.S. in Statistics from <a href="http://english.pku.edu.cn/" target="_blank">Peking University</a>.
        </p>

        <p>
             In summer 2023, I worked with <a href="https://ai.stanford.edu/~tengyuma/" target="_blank">Prof. Tengyu Ma</a> at Stanford. In summer 2022, I was an applied science intern at AWS AI. In summer 2018, I was an intern in <a href="https://www.isye.gatech.edu/" target="_blank">Industrial and Systems Engineering (ISyE)</a>, Georgia Tech, working with <a href="https://www2.isye.gatech.edu/~tzhao80/" target="_blank">Prof. Tuo Zhao</a>.
        </p>

        <p>
            My research interests are in optimization and theoretical machine learning. Recently, I am particularly interested in deep learning theory.
        </p>

        <p>
            <!--Here is my CV.-->
        </p>           
        

        <h2>Publications and Preprints</h2>
            * denotes equal contribution, (&alpha;-&beta; order) denotes alphabetical ordering
            <ul>
                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2406.01766" target="_blank">
                <b>How Does Gradient Descent Learn Features -- A Local Analysis for Regularized Two-Layer Neural Networks</b></a>
                <br>
                <b>Mo Zhou</b>, Rong Ge.<br>
                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024.<br>
                Short version appeared at NeurIPS Mathematics of Modern Machine Learning (M3L) workshop, 2023.
                </li>

                <br>
                
                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2302.00257" target="_blank">
                <b>Implicit Regularization Leads to Benign Overfitting for Sparse Linear Regression</b></a>
                <br>
                <b>Mo Zhou</b>, Rong Ge.<br>
                <em>International Conference on Machine Learning (ICML)</em>, 2023.
                </li>

                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2210.03294" target="_blank">
                <b>Understanding Edge-of-Stability Training Dynamics with a Minimalist Example</b></a>
                <br>
                Xingyu Zhu*, Zixuan Wang*, Xiang Wang, <b>Mo Zhou</b>, Rong Ge.<br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2023
                </li>

                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2304.01063" target="_blank">    
                <b>Depth-Separation with Multilayer Mean-Field Networks.</b></a>
                <br>
                Yunwei Ren, <b>Mo Zhou</b>, Rong Ge.
                <br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2023. Notable-top-25%.
                </li>

                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2210.01019" target="_blank">
                <b>Plateau in Monotonic Linear Interpolation–A “Biased” View of Loss Landscape for Deep Networks</b></a>
                <br>
                Xiang Wang, Annie N Wang, <b>Mo Zhou</b>, Rong Ge.
                <br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2023
                </li>

                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2203.03539" target="_blank">
                <b>Understanding The Robustness of Self-supervised Learning Through Topic Modeling</b></a>
                <br>
                Zeping Luo*, Shiyou Wu*, Cindy Weng*, <b>Mo Zhou</b>, Rong Ge
                <br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2023
                </li>
                
                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2106.06573" target="_blank">
                <b>Understanding Deflation Process in Over-parametrized Tensor Decomposition</b></a>
                <br>
                (&alpha;-&beta; order) Rong Ge*, Yunwei Ren*, Xiang Wang*, <b>Mo Zhou</b>*
                <br>
                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2021.
                </li>

                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2102.02410" target="_blank">
                <b>A Local Convergence Theory for Mildly Over-Parameterized Two-Layer Neural Network</b></a>
                <!--[<a href="overparam_local.pptx">slides</a>]-->
                <br>
                <b>Mo Zhou</b>, Rong Ge, Chi Jin
                <br>
                <em>Conference on Learning Theory (COLT)</em>, 2021.
                </li>

                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/1909.04653" target="_blank">
                <b>Towards Understanding the Importance of Shortcut Connections in Residual Networks</b></a>
                <br>
                Tianyi Liu*, Minshuo Chen*, <b>Mo Zhou</b>, Simon S. Du, Enlu Zhou, Tuo Zhao
                <br>
                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2019.
                </li>
                
                <br>
                
                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/1909.03172" target="_blank">
                <b>Towards Understanding the Importance of Noise in Training Neural Networks</b></a>
                <br>
                <b>Mo Zhou</b>*, Tianyi Liu*, Yan Li, Dachao Lin, Enlu Zhou, Tuo Zhao
                <br>
                <em>International Conference on Machine Learning (ICML)</em>, 2019. Long Talk.
                </li>
            </ul>    

        <h2>Presentations</h2>
            <ul>
                <li>
                    <b>A Local Convergence Theory for Mildly Over-Parameterized Two-Layer Neural Network</b><br>
                    COLT 2021, Aug. 2021<br>
                    Theory of Overparameterized Machine Learning (TOPML) 2021, Apr. 2021<br>
                    Duke Deep Learning Reading Group, Apr. 2021<br> 
                    THEORINET Journal Club/MODL Reading Group, Feb. 2021<br>
                </li>
                
            </ul> 

        <h2>Teaching</h2>
            <ul>
                <li>
                <a href="https://sites.google.com/view/duke-compsci-590-04-spring-202/home" target="_blank">
                <b>CPS590.04 Machine Learning Algorithms, 2021 Spring</b></a>. TA
                </li>

                <br>


                <li>
                <a href="https://www2.cs.duke.edu/courses/fall20/compsci330/" target="_blank">
                <b>CPS330 Design and Analysis of Algorithms, 2020 Fall</b></a>. TA
                </li>

                <br>

                <li>
                <a href="https://www2.cs.duke.edu/courses/spring20/compsci330/" target="_blank">
                <b>CPS330 Design and Analysis of Algorithms, 2020 Spring</b></a>. TA
                </li>
                
            </ul>  


        <h2>Services</h2>
            <ul>
                <li>
                Reviewer for ICML, ICLR, NeurIPS, JMLR, Mathematical Programming, STOC.
                </li>
            </ul> 

        <h2>Education</h2>
            <ul>
                <li>
                Duke University, 2019 - 2024<br>
                Ph.D. in Computer Science
                </li>
                
                <br>
                
                <li>
                Peking University, 2015 - 2019<br>
                B.S. in Statistics
                </li>
            </ul>  
    </div>


    <div id="footer">
        <p>
        Last update: Oct. 2024<br>
        <!--
        <div style="display:inline-block;width:200px;"><script type="text/javascript" src="//rf.revolvermaps.com/0/0/7.js?i=5r8n992u4ex&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;sx=0" async="async"></script></div>
        -->

        <a href="https://clustrmaps.com/site/1c1p3"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=8E8iZC4fXp9j2i9WmclWCAi6qqu2sw0xExxKmPGsIMc&cl=ffffff" /></a>
    </div>
</body>

</html>